\documentclass[12pt,a4paper,oneside]{report}
\usepackage{main}
%Begin Title
\title{
  \textbf{SIFT Object Detection in Ambiente Distribuito con Apache Hadoop}\\
  {\large Università degli Studi di Salerno}\\
  {\large Corso di Sistemi Operativi Avanzati}\\
  {\large Anno Accademico 2017-2018}\\
  \vspace{0.5cm}
  {\includegraphics[width=5cm, height=5cm]{logo_standard.png}}
}
\author{Diego Avella, Angelo Passaro, Antonio Addeo.\\{\small \textbf{Supervisore:} Prof.Giuseppe Cattaneo}}
\date{\today}
\begin{document}
  \maketitle
  %Begin Abstract
  \begin{abstract}
    Ci troviamo in una situazione storica, per quanto riguarda il mondo dell'informatica, dove enormi quantità di dati (nell'ordine dei terabyte o addirittura dei petabyte e in un  futuro non molto lontano di zettabyte) vengono processati. Questi dati devono subire analisi e trasformazioni in modo da poter essere utilizzati in settori che non riguardano solamente il mondo informatico: ambienti scientifici come l'ingegneria, la biologia, e la medicina sono i maggiori consumatori di queste informazioni e hanno bisogno di risolvere i loro "problemi" in modo semplice e il più velocemente possibile. Proprio per questo il calcolo parallelo in ambiente con memoria condivisa (shared memory) non è più grado di poter gestire e sostenere queste enormi moli di dati senza sorpassare con facilità i limiti di tempo (CPU) e di spazio (Ram e Disco) per poter rispettivamente calcolare i risultati e memorizzarli. La nascita del paradigma Map-Reduce ideato da Google e del suo file system (GFS), reso famoso dalla Apache Foundation grazie rispettivamente alle implementazioni di Hadoop e HDFS, ha risolto una grandissima branca di problemi disparati senza dover ricorrere a soluzioni artificiose che tentano di ottimizzare a livelli estremi perdendo di portabilità ma che inoltre necessitano di dover essere continuamente aggiornate per poter andare incontro ai limiti fisici di cui si è parlato prima. L'obiettivo di questa tesi è quindi quello di dimostrare che un approccio distribuito di calcolo, attraverso il modello basato su Grid Computing, permette di "superare" questi limiti in maniera semplice e con risultati accettabili se non migliori che potenzialmente possono scalare con facilità. La tesina si propone di implementare un algoritmo parallelo di Object Detection tramite estrazione di caratteristiche utilizzando l'algoritmo SIFT di Lowe. Il primo capitolo tratterà delle basi su cui si fonda il calcolo distribuito e il Grid Computing, il secondo parlerà delle componenti fondamentali che hanno permesso la nascita dei moderni centri di calcolo sia dal punto di vista hardware che software. Il terzo capitolo introduce il calcolo distribuito tramite Apache Hadoop e il modello Map-Reduce spiegandone il funzionamento interno, il quarto applica le conoscenze acquisite per implementare l'applicativo parallelo e calcolarne le prestazioni e infine ci saranno delle conclusioni finali.
  \end{abstract}
  \tableofcontents
  \listoffigures
  %Start Thesis
  \chapter{Fondamenti di Calcolo Distribuito}
  \input{./chapters/chapter_1.tex}
  \chapter{Architettura di Calcolo Distribuito}
  \input{./chapters/chapter_2.tex}
  \chapter{Calcolo Distribuito con Hadoop e Map-Reduce}
  \input{./chapters/chapter_3.tex}
  \chapter{Utilizzo di Hadoop per Object Detection }
  \input{./chapters/chapter_4.tex}
  \chapter{Conclusioni}
  \input{./chapters/chapter_5.tex}
  \input{./chapters/bibliography.tex}
\end{document}
