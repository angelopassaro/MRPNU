La Computer Vision è un settore molto complesso: Questo primo prototipo di Object Detection parallelo mostra le sue lacune ma anche dei spunti interessanti. Ci rendiamo perfettamente conto che i risultati non sono dalla nostra parte ma possiamo parzialmente giustificarli a causa della nostra inesperienza in entrambi i settori: La mancanza di librerie ottimizzate per lavorare con le immagini in Hadoop, ci ha costretto a scrivere un piccolo strato di codice necessario per trattare le immagini più agevolmente ma purtroppo non ottimizzato a supportare decenti carichi di lavoro. Anche la size del dataset ha implicato molto sui risultati generando un sovraccarico di comunicazione non necessario ma che abbiamo tenuto opportuno lasciare così perché immagini con alta risoluzione richiedono enormi matrici di float non facilmente gestibili in memoria e che avrebbero rallentano di molto l'applicativo durante il passaggio dei dati tra i map e i reduce task. Un altro fattore che ha sicuramente inficiato è il continuo leggere dall'HDFS per iterare su tutte le immagini ed effettuare il calcolo dei match. Nonostante queste lacune riteniamo l'applicativo molto interessante in quanto un approccio di calcolo distribuito basato su Hadoop riesce comunque ad ammortizzare i tempi di esecuzione e siamo certi che uno studio più approfondito del framework e di Computer Vision permetta di migliorare sensibilmente l'algoritmo. Altro spunto di miglioramento potrebbe essere l'ausilio di un database distribuito per memorizzare le features e migliorare la probabilità di matching esatto ed avere un risultato più raffinato. Infine ci sentiamo di concludere che questa esperienza con Hadoop è stata molto illuminante ed esaltante: Mettere le mani sul cluster remoto, effettuare comissioning e decomissioning, valutare le prestazioni, adattare il framework alle nostre esigenze, Usare JNI per utilizzare OpenCV ci ha permesso di capire che questo settore richiede tantissime competenze ed una conoscenza trasversale del settore informatico. 